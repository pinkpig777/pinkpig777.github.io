---
title: "A Personal Journey Toward Quant Development #00"
date: "2026-01-31"
summary: "Environment setup: Weaponizing the toolchain for high-performance C++."
tags: ["quant", "cpp", "systems"]
---

**Disclaimer** This reflects my personal thoughts and evolving understanding of the field. Some views below may be incomplete as I descend deeper into the systems programming rabbit hole.

## Goal: Eliminate Environment Friction

The goal of Week 0 is to set up a **repeatable, low-friction C++ performance environment**. In Quant Dev, you want to spend your time fighting memory layouts and cache misses, not build systems or linker errors.

### Key Requirements:
- **Modern C++**: Utilizing C++23 features.
- **Fast Incremental Builds**: Ninja-backed builds for near-instant feedback.
- **Unit Testing**: Ensuring correctness via GoogleTest.
- **Microbenchmarking**: Measuring performance via Google Benchmark.
- **Profiling**: Leveraging native macOS tooling for Apple Silicon.

---

## The Build → Test → Benchmark Loop

I have structured my environment to follow a strict pipeline. In trading, a fast program that produces the wrong answer is just an efficient way to lose money.

1.  **Change Code**
2.  **Build** (Release mode is mandatory—never benchmark Debug).
3.  **Run Unit Tests** (Validation of logic).
4.  **Run Microbenchmarks** (Validation of performance).
5.  **Compare** (Analyze deltas against previous baselines).

### Separating Correctness From Performance
I deliberately separate my project into three distinct directories:
- `src/`: Core implementation logic.
- `tests/`: Correctness and safety (GoogleTest).
- `bench/`: Speed and trade-offs (Google Benchmark).

This mirrors high-frequency trading (HFT) systems: logic is tested, hot paths are benchmarked, and the two are never allowed to contaminate each other.

---

## Learning How the Compiler Works Against You

During Week 0, I realized that modern compilers are "too smart" for naive benchmarking. If you don't explicitly tell the compiler otherwise, it will optimize away the code you are trying to measure through:
- **Dead-code elimination**: Removing code that it deems has no observable effect.
- **Constant folding**: Pre-calculating results at compile time.
- **Loop hoisting**: Moving calculations out of the timed loop.

To combat this, I now use:
- `benchmark::DoNotOptimize()`: Forces the compiler to treat a variable as "read."
- `benchmark::ClobberMemory()`: Forces the compiler to flush pending writes to memory.
- **Variable Inputs**: Using non-deterministic data to prevent the compiler from "solving" the benchmark.

---

## Toolchain Decision (macOS / Apple Silicon)

Developing on macOS requires handling the friction between the System SDK and Homebrew tools.

### Apple Clang vs. Homebrew LLVM
While Homebrew LLVM provides newer language features, it often struggles to find the macOS SDK headers (like `pthread.h` or `sched.h`) without manual configuration.
- **Default Choice**: Apple Clang (`/usr/bin/clang++`) for stability and SDK integration.
- **Opt-in Choice**: Homebrew LLVM for experimental C++26 features or Linux parity checks.

### 1. Install Core Build Tools
```bash
brew install cmake ninja pkg-config google-benchmark googletest llvm
# Ensure macOS SDK headers are available
xcode-select --install
```

### 2. Debugging & Profiling Reality Check
On Apple Silicon, the Linux-standard `perf` tool is unavailable. Instead, I utilize:
- **Xcode Instruments**: For CPU hotspots, heap allocations, and call stacks.
- **CLI Tools**: `time` for macro-benchmarking and `leaks` for memory sanity checks.

### 3. Running the Project
The build process must be strict. We use `Release` mode to ensure `-O3` and inlining are active.

```bash
# 0. Clone the project
git clone https://github.com/pinkpig777/hello-perf.git
cd hello-perf

# 1. Configure (Release mode is mandatory for benchmarks)
cmake -S . -B build -G Ninja -DCMAKE_BUILD_TYPE=Release

# 2. Build
cmake --build build

# 3. Test Correctness
ctest --test-dir build --output-on-failure
```

---

## Resetting the Mental Model

I am moving away from the "it works on my machine" mindset. In this journey, I start thinking:

> **"This code is fast because I measured it, compared it, and validated it against a baseline."**

This mindset is the foundation for everything that follows: cache locality, custom memory allocators, lock-free queues, and nanosecond latency budgets.

